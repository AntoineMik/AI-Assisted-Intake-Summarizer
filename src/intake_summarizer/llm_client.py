from __future__ import annotations
from typing import Protocol
from intake_summarizer.settings import get_settings
from intake_summarizer.schema import IntakeSummary
import os
import random
import json
import re
from dataclasses import dataclass
from typing import Optional, List, Tuple

# settings = get_settings()
FORMAT_NAME = "intake_summary"

def openai_schema_from_pydantic() -> dict:
    s = IntakeSummary.model_json_schema()

    # Force strict object behavior
    s["type"] = "object"
    s["additionalProperties"] = False

    props = s.get("properties", {})
    if not props:
        raise ValueError("Generated schema missing 'properties'.")

    # OpenAI strict mode requires required = all property keys
    s["required"] = list(props.keys())

    # Optional: remove titles/metadata that can confuse strict validators
    s.pop("title", None)
    return s

class LLMClient(Protocol):
    def summarize(self, text: str) -> str:
        """Return a JSON string that matches the IntakeSummary schema."""
        ...

# class MockLLMClient:
#     """
#     Deterministic mock client for local testing.
#     Optional chaos mode simulates real LLM failure modes.
#     """

#     def summarize(self, text: str) -> str:
#         lowered = text.lower()

#         # --- Chaos mode (opt-in via env vars) ---
#         chaos = os.getenv("MOCK_CHAOS", "0") == "1"
#         chaos_seed = os.getenv("MOCK_CHAOS_SEED")
#         chaos_rate = float(os.getenv("MOCK_CHAOS_RATE", "0.2"))  # 20% failures by default

#         if chaos:
#             if chaos_seed is not None:
#                 random.seed(int(chaos_seed))
#             r = random.random()

#             # 1) Invalid JSON
#             if r < chaos_rate / 2:
#                 return '{"not_json": '  # broken JSON on purpose

#             # 2) Schema violation: missing required field (e.g., symptoms)
#             if r < chaos_rate:
#                 return """{
#                     "chief_complaint": "Intake summary",
#                     "duration": "unknown",
#                     "urgency": "unknown",
#                     "triage_category": "unknown",
#                     "red_flags": [],
#                     "recommended_next_step": "Schedule clinician review; escalate if symptoms worsen.",
#                     "confidence": 0.62,
#                     "notes": "CHAOS: missing symptoms"
#                 }"""

#         # --- Normal deterministic behavior ---
#         urgency = "unknown"
#         red_flags = []
#         if "chest pain" in lowered and ("shortness of breath" in lowered or "trouble breathing" in lowered):
#             urgency = "emergency"
#             red_flags.append("Possible cardiopulmonary emergency symptoms.")
#         elif "severe" in lowered or "faint" in lowered:
#             urgency = "urgent"

#         return """{
#             "chief_complaint": "Intake summary",
#             "symptoms": ["reported symptoms in free text"],
#             "duration": "unknown",
#             "urgency": "%s",
#             "triage_category": "unknown",
#             "red_flags": %s,
#             "recommended_next_step": "Schedule clinician review; escalate if symptoms worsen.",
#             "confidence": 0.62,
#             "notes": "Generated by mock client; replace with real LLM provider."
#         }""" % (urgency, str(red_flags).replace("'", '"'))

# class MockLLMClient:
#     """
#     Deterministic mock client for local testing.
#     Optional chaos mode simulates malformed JSON to test retries/failure handling.
#     """

#     def __init__(
#         self,
#         chaos_enabled: bool = False,
#         chaos_rate: float = 0.0,
#         chaos_seed: Optional[int] = None,
#     ) -> None:
#         self.chaos_enabled = chaos_enabled
#         self.chaos_rate = float(chaos_rate or 0.0)
#         self.rng = random.Random(chaos_seed) if chaos_seed is not None else random.Random()

#     def summarize(self, text: str) -> str:
#         lowered = text.lower()
#         urgency = "unknown"
#         red_flags = []

#         if "chest pain" in lowered and ("shortness of breath" in lowered or "trouble breathing" in lowered):
#             urgency = "emergency"
#             red_flags.append("Possible cardiopulmonary emergency symptoms.")
#         elif "severe" in lowered or "faint" in lowered:
#             urgency = "urgent"

#         payload = {
#             "chief_complaint": "Intake summary",
#             "symptoms": ["reported symptoms in free text"],
#             "duration": "unknown",
#             "urgency": urgency,
#             "triage_category": "unknown",
#             "red_flags": red_flags,
#             "recommended_next_step": "Schedule clinician review; escalate if symptoms worsen.",
#             "confidence": 0.62,
#             "notes": "Generated by mock client; replace with real LLM provider.",
#         }

#         out = json.dumps(payload)

#         # chaos: corrupt output sometimes
#         if self.chaos_enabled and self.chaos_rate > 0:
#             if self.rng.random() < self.chaos_rate:
#                 return out[:13] + "<<<CORRUPT>>>"  # guaranteed invalid JSON

#         return out


# -----------------------------
# Deterministic heuristic mock
# -----------------------------

_DURATION_PATTERNS: List[Tuple[str, str]] = [
    (r"\b(today)\b", "today"),
    (r"\b(yesterday)\b", "yesterday"),
    (r"\b(\d+)\s*(minute|minutes|min|mins)\b", r"\1 minutes"),
    (r"\b(\d+)\s*(hour|hours|hr|hrs)\b", r"\1 hours"),
    (r"\b(\d+)\s*(day|days)\b", r"\1 days"),
    (r"\b(\d+)\s*(week|weeks)\b", r"\1 weeks"),
    (r"\b(\d+)\s*(month|months)\b", r"\1 months"),
    (r"\ba few days\b", "a few days"),
    (r"\b(several days)\b", "several days"),
]

# Phrase buckets for deterministic matching
CHEST_TERMS = (
    "chest pain",
    "chest tightness",
    "chest pressure",
    "tightness in chest",
    "pressure in chest",
)

SOB_TERMS = (
    "shortness of breath",
    "trouble breathing",
    "difficulty breathing",
    "breathless",
    "can't breathe",
    "cant breathe",
)

NEURO_RED_FLAGS = (
    "one-sided weakness",
    "facial droop",
    "slurred speech",
    "confusion",
    "seizure",
    "worst headache",
)

BLEEDING_RED_FLAGS = (
    "vomiting blood",
    "blood in vomit",
    "black stools",
    "bloody stool",
    "heavy bleeding",
)

PREGNANCY_RED_FLAGS = (
    "pregnant",
    "pregnancy",
    "vaginal bleeding",
)

SEVERE_TERMS = (
    "severe",
    "fainted",
    "passed out",
    "syncope",
)

TELEHEALTH_HINTS = (
    "video visit",
    "telehealth",
    "virtual visit",
    "phone visit",
)

SELF_CARE_HINTS = (
    "seasonal allergies",
    "runny nose",
    "nasal congestion",
    "mild sore throat",
)

# Keep symptom phrases relatively short and human-readable
SYMPTOM_PHRASES = [
    ("chest pain", "chest pain"),
    ("chest tightness", "chest tightness"),
    ("chest pressure", "chest pressure"),
    ("shortness of breath", "shortness of breath"),
    ("trouble breathing", "trouble breathing"),
    ("difficulty breathing", "difficulty breathing"),
    ("sore throat", "sore throat"),
    ("runny nose", "runny nose"),
    ("nasal congestion", "nasal congestion"),
    ("fever", "fever"),
    ("cough", "cough"),
    ("headache", "headache"),
    ("dizziness", "dizziness"),
    ("nausea", "nausea"),
    ("vomiting", "vomiting"),
    ("abdominal pain", "abdominal pain"),
    ("rash", "rash"),
]


def _extract_duration(text: str) -> str:
    lowered = text.lower()
    for pat, repl in _DURATION_PATTERNS:
        m = re.search(pat, lowered)
        if m:
            if "\\" in repl:  # replacement template
                return re.sub(pat, repl, m.group(0))
            return repl
    return "unknown"


def _extract_symptoms(text: str, max_items: int = 10) -> List[str]:
    lowered = text.lower()
    found: List[str] = []
    for needle, label in SYMPTOM_PHRASES:
        if needle in lowered and label not in found:
            found.append(label)
        if len(found) >= max_items:
            break
    return found


def _chief_complaint(symptoms: List[str]) -> str:
    if not symptoms:
        return "Intake summary"
    # Prefer a concise headline
    if "chest pain" in symptoms or "chest tightness" in symptoms or "chest pressure" in symptoms:
        if "shortness of breath" in symptoms or "trouble breathing" in symptoms or "difficulty breathing" in symptoms:
            return "Chest symptoms with breathing difficulty"
        return "Chest symptoms"
    return " / ".join(symptoms[:2]).title()


def _contains_any(text: str, terms: Tuple[str, ...]) -> bool:
    t = text.lower()
    return any(x in t for x in terms)


def _build_red_flags(text: str) -> List[str]:
    t = text.lower()
    flags: List[str] = []

    # Cardio/pulm
    if _contains_any(t, CHEST_TERMS) and _contains_any(t, SOB_TERMS):
        flags.append("Chest symptoms with shortness of breath.")

    # Neuro
    if any(x in t for x in NEURO_RED_FLAGS):
        flags.append("Possible acute neurologic symptoms.")

    # Bleeding
    if any(x in t for x in BLEEDING_RED_FLAGS):
        flags.append("Possible significant bleeding symptoms.")

    # Pregnancy + bleeding (simple)
    if any(x in t for x in PREGNANCY_RED_FLAGS):
        # keep it conservative; don’t “diagnose”
        flags.append("Pregnancy-related concern mentioned.")

    # Severity cues
    if any(x in t for x in SEVERE_TERMS):
        flags.append("Severe symptom indicator present (e.g., fainting/severe).")

    return flags[:10]


def _urgency_from_text(text: str, red_flags: List[str]) -> str:
    t = text.lower()

    # Emergency triggers
    if _contains_any(t, CHEST_TERMS) and _contains_any(t, SOB_TERMS):
        return "emergency"
    if any(x in t for x in NEURO_RED_FLAGS):
        return "emergency"
    if any(x in t for x in BLEEDING_RED_FLAGS):
        return "emergency"

    # Urgent triggers
    if any(x in t for x in SEVERE_TERMS):
        return "urgent"
    if "worse" in t and ("days" in t or "week" in t or "hours" in t):
        return "urgent"

    # Routine triggers
    if any(x in t for x in SELF_CARE_HINTS):
        return "routine"

    return "unknown"


def _triage_from_text(text: str, urgency: str) -> str:
    t = text.lower()

    if urgency == "emergency":
        return "in_person"

    if any(x in t for x in TELEHEALTH_HINTS):
        return "telehealth"

    if any(x in t for x in SELF_CARE_HINTS):
        return "self_care"

    # default conservative routing
    return "unknown"


def _next_step(urgency: str, triage: str) -> str:
    if urgency == "emergency":
        return "Seek emergency care immediately or call emergency services."
    if urgency == "urgent":
        return "Schedule same-day clinician evaluation; seek urgent care if worsening."
    if triage == "telehealth":
        return "Schedule a telehealth clinician review; escalate if symptoms worsen."
    if triage == "self_care":
        return "Provide self-care guidance and advise follow-up if symptoms persist or worsen."
    return "Schedule clinician review; escalate if symptoms worsen."


class MockLLMClient:
    """
    Deterministic mock client for local testing and demos.
    - Produces realistic-ish JSON matching IntakeSummary.
    - Optional chaos mode can corrupt JSON to test retries/failure handling.
    """

    def __init__(
        self,
        chaos_enabled: bool = False,
        chaos_rate: float = 0.0,
        chaos_seed: Optional[int] = None,
    ) -> None:
        self.chaos_enabled = bool(chaos_enabled)
        self.chaos_rate = float(chaos_rate or 0.0)
        self.rng = random.Random(chaos_seed) if chaos_seed is not None else random.Random()

    def summarize(self, text: str) -> str:
        symptoms = _extract_symptoms(text)
        duration = _extract_duration(text)
        red_flags = _build_red_flags(text)
        urgency = _urgency_from_text(text, red_flags)
        triage = _triage_from_text(text, urgency)

        payload = {
            "chief_complaint": _chief_complaint(symptoms),
            "symptoms": symptoms,
            "duration": duration,
            "urgency": urgency,
            "triage_category": triage,
            "red_flags": red_flags,
            "recommended_next_step": _next_step(urgency, triage),
            "confidence": 0.62,
            "notes": "Generated by mock client; deterministic heuristics (not a clinical decision).",
        }

        out = json.dumps(payload)

        # Chaos: corrupt output sometimes (invalid JSON)
        if self.chaos_enabled and self.chaos_rate > 0 and self.rng.random() < self.chaos_rate:
            return out[:13] + "<<<CORRUPT>>>"

        return out

SCHEMA = {
    "type": "object",
    "additionalProperties": False,
    "properties": {
        "chief_complaint": {"type": "string"},
        "symptoms": {"type": "array", "items": {"type": "string"}},
        "duration": {"type": "string"},
        "urgency": {"type": "string", "enum": ["emergency", "urgent", "routine", "unknown"]},
        "triage_category": {"type": "string", "enum": ["telehealth", "in_person", "self_care", "unknown"]},
        "red_flags": {"type": "array", "items": {"type": "string"}},
        "recommended_next_step": {"type": "string"},
        "confidence": {"type": "number"},
        "notes": {"type": "string"},
    },
    "required": [
        "chief_complaint",
        "symptoms",
        "duration",
        "urgency",
        "triage_category",
        "red_flags",
        "recommended_next_step",
        "confidence",
        "notes",
    ],
}


class OpenAILLMClient:
    def __init__(self) -> None:
        from openai import OpenAI  # ✅ lazy import
        import httpx
        settings = get_settings()

        if not settings.openai_api_key:
            raise ValueError("OPENAI_API_KEY is not set.")
        self.client = OpenAI(timeout=httpx.Timeout(30.0, connect=10.0))

    def summarize(self, text: str) -> str:
        # JSON Schema that matches your Pydantic IntakeSummary

        settings = get_settings()

        system_prompt = (
            "You generate conservative clinical intake summaries.\n"
            "Do NOT invent diagnoses, medications, vitals, or history.\n"
            "If unknown, use 'unknown' or empty lists.\n"
            "Return ONLY JSON that matches the provided schema."
        )

        resp = self.client.responses.create(
            model=settings.llm_model,
            input=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text},
            ],
            text={
                "format": {
                    "type": "json_schema", 
                    "name": FORMAT_NAME, 
                    "strict": True,
                    "schema": openai_schema_from_pydantic(),
                }
            },
            temperature=0,
            # Responses are stored by default; disable storage for sensitive intake text
            store=False,
        )

        out = (resp.output_text or "").strip()
        if not out:
            raise ValueError("OpenAI returned empty output_text.")
        return out