from typing import Protocol
from intake_summarizer.settings import get_settings
from intake_summarizer.schema import IntakeSummary
import os
import random


settings = get_settings()
FORMAT_NAME = "intake_summary"

def openai_schema_from_pydantic() -> dict:
    s = IntakeSummary.model_json_schema()

    # Force strict object behavior
    s["type"] = "object"
    s["additionalProperties"] = False

    props = s.get("properties", {})
    if not props:
        raise ValueError("Generated schema missing 'properties'.")

    # OpenAI strict mode requires required = all property keys
    s["required"] = list(props.keys())

    # Optional: remove titles/metadata that can confuse strict validators
    s.pop("title", None)
    return s

class LLMClient(Protocol):
    def summarize(self, text: str) -> str:
        """Return a JSON string that matches the IntakeSummary schema."""
        ...

class MockLLMClient:
    """
    Deterministic mock client for local testing.
    Optional chaos mode simulates real LLM failure modes.
    """

    def summarize(self, text: str) -> str:
        lowered = text.lower()

        # --- Chaos mode (opt-in via env vars) ---
        chaos = os.getenv("MOCK_CHAOS", "0") == "1"
        chaos_seed = os.getenv("MOCK_CHAOS_SEED")
        chaos_rate = float(os.getenv("MOCK_CHAOS_RATE", "0.2"))  # 20% failures by default

        if chaos:
            if chaos_seed is not None:
                random.seed(int(chaos_seed))
            r = random.random()

            # 1) Invalid JSON
            if r < chaos_rate / 2:
                return '{"not_json": '  # broken JSON on purpose

            # 2) Schema violation: missing required field (e.g., symptoms)
            if r < chaos_rate:
                return """{
                    "chief_complaint": "Intake summary",
                    "duration": "unknown",
                    "urgency": "unknown",
                    "triage_category": "unknown",
                    "red_flags": [],
                    "recommended_next_step": "Schedule clinician review; escalate if symptoms worsen.",
                    "confidence": 0.62,
                    "notes": "CHAOS: missing symptoms"
                }"""

        # --- Normal deterministic behavior ---
        urgency = "unknown"
        red_flags = []
        if "chest pain" in lowered and ("shortness of breath" in lowered or "trouble breathing" in lowered):
            urgency = "emergency"
            red_flags.append("Possible cardiopulmonary emergency symptoms.")
        elif "severe" in lowered or "faint" in lowered:
            urgency = "urgent"

        return """{
            "chief_complaint": "Intake summary",
            "symptoms": ["reported symptoms in free text"],
            "duration": "unknown",
            "urgency": "%s",
            "triage_category": "unknown",
            "red_flags": %s,
            "recommended_next_step": "Schedule clinician review; escalate if symptoms worsen.",
            "confidence": 0.62,
            "notes": "Generated by mock client; replace with real LLM provider."
        }""" % (urgency, str(red_flags).replace("'", '"'))
    

SCHEMA = {
    "type": "object",
    "additionalProperties": False,
    "properties": {
        "chief_complaint": {"type": "string"},
        "symptoms": {"type": "array", "items": {"type": "string"}},
        "duration": {"type": "string"},
        "urgency": {"type": "string", "enum": ["emergency", "urgent", "routine", "unknown"]},
        "triage_category": {"type": "string", "enum": ["telehealth", "in_person", "self_care", "unknown"]},
        "red_flags": {"type": "array", "items": {"type": "string"}},
        "recommended_next_step": {"type": "string"},
        "confidence": {"type": "number"},
        "notes": {"type": "string"},
    },
    "required": [
        "chief_complaint",
        "symptoms",
        "duration",
        "urgency",
        "triage_category",
        "red_flags",
        "recommended_next_step",
        "confidence",
        "notes",
    ],
}


class OpenAILLMClient:
    def __init__(self) -> None:
        from openai import OpenAI  # âœ… lazy import
        import httpx

        if not settings.openai_api_key:
            raise ValueError("OPENAI_API_KEY is not set.")
        self.client = OpenAI(timeout=httpx.Timeout(30.0, connect=10.0))

    def summarize(self, text: str) -> str:
        # JSON Schema that matches your Pydantic IntakeSummary

        system_prompt = (
            "You generate conservative clinical intake summaries.\n"
            "Do NOT invent diagnoses, medications, vitals, or history.\n"
            "If unknown, use 'unknown' or empty lists.\n"
            "Return ONLY JSON that matches the provided schema."
        )

        resp = self.client.responses.create(
            model=settings.llm_model,
            input=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text},
            ],
            text={
                "format": {
                    "type": "json_schema", 
                    "name": FORMAT_NAME, 
                    "strict": True,
                    "schema": openai_schema_from_pydantic(),
                }
            },
            temperature=0,
            # Responses are stored by default; disable storage for sensitive intake text
            store=False,
        )

        out = (resp.output_text or "").strip()
        if not out:
            raise ValueError("OpenAI returned empty output_text.")
        return out